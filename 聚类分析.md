



# 频繁项集

​	数据的购物篮模型用于描述两类对象之间一种常见形式的多对多关系。一个在多个购物篮中出现的项集称为频繁项集。支持度：如果I是一个项集，I的支持度是指I包含的购物篮数目(一个词语在某个购物篮出现两次不会被考虑)，如果大于阈值，则称I是频繁项集。

​	若一个双元素集合是频繁项集，则双元素集合中的两个元素本身都必须是频繁的，这样集合才有可能是频繁的。

​	频繁项集分析的应用并不限于购物篮分析，同样的模型可以用于挖掘很多其他类型的数据，例如关联概念：词是项，购物篮是文档。

## A-priori算法

由于项对的数目太多而无法在内存中对所有项对技术，A-priori算法被设计成能够减少必须计数的项对数目，当然其代价是做两遍扫描而不是一遍扫描。

## 层次聚类

层次聚类的问题：

- 簇如何表示
- 如何选择哪两个簇进行合并
- 簇合并何时结束

​       对于欧式空间，可以用簇质心或者簇内平均点来表示一个簇，对单点组成的簇，该点就是簇质心；将簇之间距离定义为其质心之间的欧式距离，并选择具有最短距离的两个簇进行合并，并选择具有最短距离的两个簇进行合并，簇之间距离也存在其他定义，也可以不基于距离而基于其他方式选择最好的两个簇进行合并。

​	合并结束：

- 事先确信数据中簇的数目

- 在某个点，现有簇的最佳合并会产生你个不恰当的簇时停止合并，例如要求一个簇内所有点到其质心的平均距离必须小于某个上界

  距离方式

- 两个簇的距离为两个簇中所有点之间的最短距离

- 两个簇的距离为两个簇中所有点对之间距离的平均值

- 簇的半径：簇内所有点到质心的最大距离

- 簇的直径：簇内任意两个点之间的最大距离





## KNN

### 1.基本原理

​	KNN算法又称为K最近邻分类算法；所谓k最近邻，就是指最接近的k个邻居（数据），即每个样本都可以由它的K个邻居来表达（最大表决法）。

​	核心思想：在一个包含未知样本的空间，可以根据离这个样本最邻近的k个样本的数据类型来确定样本的数据类型。

​	该算法设计三个主要因素：训练集、距离与相似的度量、k的大小

​	主要是for循环，在循环中将k个选择点之外的每个点分配给最近的簇，这里的最近指簇心最近；当点分配到簇之后簇的质心可能会偏移。但是由于只有簇附近的点才可能被分配给自己，所以簇的质心也不会移动太大。

### 2.算法特点

​	KNN主要使用加权投票法，即根据距离的远近，对近邻的投票进行加权，距离越近权重越大（权重为距离平方的倒数）

优点：

- 易于实现，无需估计参数，无需训练，支持增量学习，能对超多边形的复杂决策空间建模；
- 可用于数值型数据和离散型数据
- 对异常值不敏感

缺点

- 计算量大，分析速度慢，需要大量内存
- 样本不平衡问题
- 无法给出数据真正含义

### 3.常见问题

#### k值问题

​	k太小，分类结果容易受噪声点影响；k太大，近邻中有可能包含太多的其他类别的特点（对距离进行加权，通常可以降低k值对算法的影响）。经验规则：k一般低于训练样本数的平方根。

#### 类别如何判别更加合适

​	投票法没有考虑近邻的距离的远近，距离更近的近邻也许更应该决定最终的分类。

#### 如何选择合适的距离度量

​	高纬度对距离衡量的影响：众所周知当变量数越多，欧式距离的区分能力就越差。

​	变量值域对距离影响：值域越大的变量常常会在距离计算中占据主导作用，因此应先对变量进行标准化。

#### 训练样本是否要一视同仁

​	在训练集中，有些样本可能是值得依赖的。可以给不同的样本不同的权重，加强依赖样本的权重，降低不可信样本的影响。

#### KNN性能问题

​	KNN是一种懒惰算法：构造模型很简单，但是对测试样本分类地的开销大，因为要扫描全部训练样本并计算距离。



## CURE算法

​	CURE对簇的形状没有任何假设，簇不需要满足正太分布，CURE使用一些代表点的集合来代表簇。

初始化：

- 抽取一部分样本数据在内存中进行聚类。
- 从每个簇中选择一小部分点集作为簇的代表点