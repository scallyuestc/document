

# 树模型





## 决策树

#### 1.常见问题

##### 1、什么是决策树？

- if-then规则的集合，该集合是决策树上所有从根节点到叶节点的路径集合
- 定义在特征空间与类空间上的条件概率分布，决策树实际上讲特征空间划分成了互不相交的单元，每个从根节点到叶节点对应着一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。哪个类别有较高的条件概率，就把该单元中的实例强行划分为该类别

##### 2、如何学习一颗决策树



## GBDT

#### 1.原理

回归问题：https://blog.csdn.net/qq_22238533/article/details/79185969

分类问题：https://blog.csdn.net/qq_22238533/article/details/79192579

多分类问题：https://blog.csdn.net/qq_22238533/article/details/79199605

#### 2.常见问题

##### 1、gbdt算法的流程

​	gbdt通过多轮迭代，每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的残差基础上进行训练，对弱分类器的要求一般是足够简单，并且是低方差和高偏差的。因为训练过程中是通过降低偏差来不断提高最终分类器的精度。

​	弱分类器一般选择为·CART（分类回归树）。由于高偏差要求，每个分类回归树的深度不会很深。最终的强分类器是将每轮训练得到的弱分类器加权求和得到的。

![1](C:\Users\scall\Desktop\面试准备\图片\1.png)

分类问题损失函数：

- 指数损失：原理和Adaboost一样
- 对数损失：分为二元分类和多元分类

回归问题损失函数：

- 均方差（MSE）：L(y,f(x))=(y−f(x))2

- 绝对损失（MAE)：L(y,f(x))=|y−f(x)|

- Huber损失：均方差和绝对损失的折中产物，对于原理中心的异常点，采用绝对损失，

  对于分类问题损失函数有指数损失函数和对数损失函数；对于回归问题，损失函数有均方差，绝对损失，Huber损失：是均方差和绝对损失的折中产物，对于中心附近的点采用均方差。





![2](C:\Users\scall\Desktop\面试准备\图片\2.png)



##### 2、gbdt如何选择特征

​	gbdt选择特征其实是CART Tree生成的过程。

​	CART Tree生成过程：是一个特征选择的过程，假设目前特征总共有M个特征，第一步我们需要从中选择出一个特征j，作为二叉树的第一个节点；然后对特征j选择一个切分点m，一个样本的特征值如果小于M，则分为一类，如果大于m，则分为另一类。其他节点生成过程类似.

​	选择特征j：首先遍历每个特征，然后对每个特征遍历它多有可能切分点，找到最优特征和切分点。

##### 3、gbdt如何构造特征

​	其实说gbdt 能够构建特征并非很准确，gbdt 本身是不能产生特征的，但是我们可以利用gbdt去产生特征的组合。在CTR预估中，工业界一般会采用[逻辑回归](http://www.cnblogs.com/ModifyRong/p/7739955.html#3825035)去进行处理,在我的上一篇博文当中已经说过，逻辑回归本身是适合处理线性可分的数据，如果我们想让逻辑回归处理非线性的数据，其中一种方式便是组合不同特征，增强逻辑回归对非线性分布的拟合能力。

​        长久以来，我们都是通过人工的先验知识或者实验来获得有效的组合特征，但是很多时候，使用人工经验知识来组合特征过于耗费人力，造成了机器学习当中一个很奇特的现象：有多少人工就有多少智能。关键是这样通过人工去组合特征并不一定能够提升模型的效果。所以我们的从业者或者学界一直都有一个趋势便是通过算法自动，高效的寻找到有效的特征组合。Facebook 在2014年 发表的一篇论文便是这种尝试下的产物，利用gbdt去产生有效的特征组合，以便用于逻辑回归的训练，提升模型最终的效果。

##### 4、gbdt 通过什么方式减少误差 ？

​	gbdt让损失函数沿着梯度方向快速下降。利用损失函数的负梯度在当前模型的值作为提升树算法中参数的近似值取拟合一个回归树，使得损失函数能够不断的快速下降。gbdt在每轮迭代的时候，都去拟合损失函数在当前模型下的负梯度。

​	这样每轮训练的时候能够让损失函数尽可能的减小，尽快的收敛达到局部最优解或者全局最优解。

##### 5、gbdt如何应用于分类问题

​	gbdt无论用于分类还是回归一直都是在使用CART Tree。gbdt使用对数损失函数或者指数损失函数来度量上一轮训练的残差。除了初始化、负梯度、叶子节点，其他的和gbdt在回归问题上的应用没有区别。对于多分类问题，gbdt采用一对多的策略，在每轮训练时生成k（k为类别）个树，当拟合完k个树的时候采取进行下一轮的训练，为什么不能先把某一类别的M棵树学完，再去学习另一个类别？还不知道



##### 6、gdbt相对于传统的LR、svm效果好一些

分别从多个角度进行解释：

- 模型角度：当选择一个假设空间后，在训练集训练数据，从而逼近理想判别函数，对于LR、SVM这样的函数，如果拟合的不好，可以增加特征或映射到高维空间取得足够的正确率，对于Tree Enseable 是通过训练更多的弱分类器，即LR、SVM、GBDT都可以通过某种方式在训练集取得足够效果，但是在特征不变的情况下，GBDT能够取得更好的效果，即对训练数据而言GBDT对特征数量的要求没有那么高；对于测试数据而言，LR SVM在训练集上拟合好的时候，容易发生过拟合的情况，而对于Tree enseable模型而言：boosting能够降低偏差，同时由于每个弱分类器是非常简单的，在实际中很不容易发生过拟合，bagging能够降低方差，更不容易发生过拟合。对于Tree enseable加树是为了降低训练集误差，树的深度会增加模型的复杂度。
- 自适应非线性角度：随着决策树的增长，能够产生高度非线性的模型，而SVM等线性模型的非线性需要基于核方法，要求在学习之前就订好核函数，然而确定核函数不容易。
- 参数与非参数模型角度：lr svm是参数模型，而tree enseable是非参数模型，如果数据不符合lr或者svm假设，则效果可能会变差。
- 多分类器隐含正则：对于bagging来说，有差异的学习器的组合能够起到一定的正则化的作用，从而使得总体复杂度降低，提高泛化能力，直观上讲，由于拟合到不同的地方，总体投票或平均后并不会过拟合。
- 数据角度：实际中数据多少是有噪声的，基于数的算法抗噪能力更强。比如在树模型中，很容易对缺失值进行处理
- 特征多样性：svm本质上属于一个几何模型，这个模型需要去定义不同样本特征之间的kernel或者similarity，对多样性的特征定义similarity比较难，而对树模型来说，并不是太关注特征之间的simality，使得树模型更适合处理多样性的特征，对特征分布不是那么敏感。

##### 6、gdbt如何调参

第一类：GBDT类boosting框架参数

- n_estimators: 弱学习器的最大迭代次数，或者说最大的弱学习器的个数
- learning_rate: 即每个弱学习器的权重缩减系数，对于同样的训练集拟合效果，较小的学习率意味需要更多地弱学习器的迭代次数。通常将n_estimators和learning_rate结合起来进行考虑。
- subsample:随机森林使用的是放回抽样，GBDT是不放回抽样，选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差。
- loss：GBDT中的损失函数
- alpha：这个参数只有回归才有

第二类：GBDT弱学习器参数（CART Tree）

- 划分时考虑的最大特征数max_features
- 决策树最大深度max_depth：
- 内部节点再划分所需最小样本数min_samples_split：限制子树继续划分的条件，如果节点样本数小于这个值，则不继续选择特征进行划分。
- 叶子节点最少样本数min_samples_leaf：限制叶子节点最少的样本数，如果某叶子节点样本数小于该值，在将其和兄弟节点一起剪枝
- 最大叶子节点数max_leaf_nodes:限制叶子节点数目，可以防止过拟合
- 节点划分最小不纯度min_impurity_split：这个值限制了决策树的增长，如果节点的不纯度小于阈值，则该节点不在划分。

##### 7、gbdt算法特征重要度计算（tree enseable通用）

​	特征j的全局重要度通过特征j在单个树中重要度的平均值来衡量。

​	单个树中特征重要度：非叶节点中，如果某个非叶节点关联特征不为该特征，则该节点对重要度贡献为0，否则共现为该节点分裂后平方损失的减少值。

##### 8、单颗数的停止生长条件

- 节点分裂时的最小样本数
- 最大深度
- 最多叶子节点数
- 损失函数满足一定约束条件（不太清楚）

##### 9、当增加树的颗数时，训练时长是线性增长的么？

不是，每颗树的生成时间复杂度不一样

##### 10、当增加一棵树的叶子节点数目时，训练时长是线性增加么？

不是，叶子节点数和每个树的生成时间复杂度不成正比

##### 11、每个节点保存什么信息？

分裂节点保存分割特征信息（特征，分割值），叶子节点保存的是预测值

##### 12、**gbdt在训练和预测的时候都用到了步长，这两个步长一样么？都有什么用，如果不一样，为什么？怎么设步长的大小？（太小？太大？）在预测时，设太大对排序结果有什么影响？跟shrinking里面的步长一样么？**



**这两个步长一样么？**答：训练跟预测时，两个步长是一样的，也就是预测时的步长为训练时的步长，从训练的过程可以得知（更新当前迭代模型的时候）。
**都有什么用，如果不一样，为什么？**答：它的作用就是使得每次更新模型的时候，使得loss能够平稳地沿着负梯度的方向下降，不至于发生震荡。
**那么怎么设步长的大小?**
答：有两种方法，一种就是按**策略**来决定步长，另一种就是在训练模型的同时，**学习**步长。
A. 策略：
a 每个树步长恒定且相等，一般设较小的值；
b 开始的时候给步长设一个较小值，随着迭代次数动态改变，或者说衰减。
B. 学习：
因为在训练第k棵树的时候，前k-1棵树时已知的，而且求梯度的时候是利用前k-1棵树来获得。所以这个时候，就可以把步长当作一个变量来学习。
**（太小？太大？）在预测时，对排序结果有什么影响？**
答：如果步长过大，在训练的时候容易发生震荡，使得模型学不好，或者完全没有学好，从而导致模型精度不好。
而步长过小，导致训练时间过长，即迭代次数较大，从而生成较多的树，使得模型变得复杂，容易造成过拟合以及增加计算量。
不过步长较小的话，使训练比较稳定，总能找到一个稳定的局部最优解。
个人觉得过大过小的话，模型的预测值都会偏离真实情况（可能比较严重），从而导致模型精度不好。
**跟shrinking里面的步长一样么？**答：这里的步长跟shrinking里面的步长是一致的。

##### 13、boosting的本意是是什么？跟bagging，random forest，adaboost，gradient boosting有什么区别？

答：**Bagging:**       可以看成是一种圆桌会议，或是投票选举的形式。通过训练多个模型，将这些训练好的模型进行加权组合来获得最终的输出结果(分类/回归)，一般这类方法的效果，都会好于单个模型的效果。在实践中，在特征一定的情况下，大家总是使用Bagging的思想去提升效果。例如kaggle上的问题解决，因为大家获得的数据都是一样的，特别是有些数据已经过预处理。       基本的思路比较简单，就是：训练时，使用replacement的sampling方法，sampling一部分训练数据k次并训练k个模型；预测时，使用k个模型，如果为分类，则让k个模型均进行分类并选择出现次数最多的类（每个类出现的次数占比可以视为置信度）；如为回归，则为各类器返回的结果的平均值。在该处，Bagging算法可以认为每个分类器的权重都一样由于每次迭代的采样是独立的，所以bagging可以并行。
而boosting的采样或者更改样本的权重依赖于上一次迭代的结果，在迭代层面上是不能并行的。
**Random forest：**
​       随机森林在bagging的基础上做了修改。
A. 从样本集散用Boostrap采样选出n个样本，预建立CART
B. 在树的每个节点上，从所有属性中随机选择k个属性/特征，选择出一个最佳属性/特征作为节点
C. 重复上述两步m次，i.e.build m棵cart
D. 这m棵cart形成random forest。       随机森林可以既处理属性是离散的量，比如ID3算法，也可以处理属性为连续值得量，比如C4.5算法。
这里的random就是指：
A. boostrap中的随机选择样本
B. random subspace的算法中从属性/特征即中随机选择k个属性/特征，每棵树节点分裂时，从这随机的k个属性/特征，选择最优的。
**Boosting:**
boosting是”提升”的意思。一般Boosting算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。boosting在选择hyperspace的时候给样本加了一个权值，使得loss function尽量考虑那些分错类的样本（如分错类的样本weight大）。怎么做的呢？
boosting重采样的不是样本，而是样本的分布，对于分类正确的样本权值低，分类错误的样本权值高(通常是边界附近的样本)，最后的分类器是很多弱分类器的线性叠加(加权组合)。
或者这么理解也是可以的:
如果弱学习器与强学习器是等价的, 当强学习器难以学习时(如强学习器高度非线性等)，问题就可以转化为这样的学习问题：
学习多个弱分类器(弱分类器容易学习)，并将多个弱分类器组合成一个强分类器(与原来的强学习器等价)。
**Adaboosting:**
​       这其实思想相当的简单，大概是对一份数据，建立M个模型(比如分类)，而一般这种模型比较简单，称为弱分类器(weak learner)。每次分类都将上一次分错的数据权重提高一点，对分对的数据权重降低一点，再进行分类。这样最终得到的分类器在测试数据与训练数据上都可以得到比较好的效果。
​       每次迭代的样本是一样的，即没有采样过程，不同的是不同的样本权重不一样。(当然也可以对样本/特征进行采样，这个不是adaboosting的原意)。
另外，每个分类器的步长由在训练该分类器时的误差来生成。
**Gradient boosting: **
​       每一次的计算是为了减少上一次的残差(residual)，而为了消除残差，我们可以在残差减少的梯度 (Gradient)方向上建立一个新的模型。所以说在Gradient Boost中，每个新模型是为了使之前模型的残差往梯度方向减少，与传统Boost对正确，错误的样本进行加权有着很大的区别。(或者这样理解：每一次建立模型是在之前建立模型损失函数的梯度下降方向。这句话有一点拗口，损失函数(loss function)描述的是模型的不靠谱程度，损失函数越大，则说明模型越容易出错(其实这里有一个方差、偏差均衡的问题, 但是这里就假设损失函数越大, 模型越容易出错)。如果我们的模型能够让损失函数持续的下降, 则说明我们的模型在不停的改进, 而最好的方式就是让损失函数在其Gradient的方向上下降)。



##### 14、GDBT中哪些部分可以并行？

- 计算每个样本的负梯度
- 分裂挑选最佳特征及最优分割点，对特征计算响应的误差即均方差
- 更新每个样本的负梯度
- 最后预测过程中、每个样本将之前所有树的结果累加

##### 15、树畸形生长会带来哪些危害，如何预防？



##### 16、GBDT正则化

​	和Adaboost一样，我们也需要对GBDT进行正则化，防止过拟合。GBDT的正则化主要有三种方式。

　　第一种是和Adaboost类似的正则化项，即步长(learning rate)。定义为νν,对于前面的弱学习器的迭代

​		fk(x) = fk−1(x) + hk(x)

　　如果我们加上了正则化项，则 fk(x) = fk−1(x) + ν hk(x)

　　ν的取值范围为0<ν≤10<ν≤1。对于同样的训练集学习效果，较小的ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。

 

　　第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。

　　使用了子采样的GBDT有时也称作随机梯度提升树(Stochastic Gradient Boosting Tree, SGBT)。由于使用了子采样，程序可以通过采样分发到不同的任务去做boosting的迭代过程，最后形成新树，从而减少弱学习器难以并行学习的弱点。

 	第三种是对于弱学习器即CART回归树进行正则化剪枝。在决策树原理篇里我们已经讲过，这里就不重复了。

##### 17、自采样的SGBT是什么？怎样实现并行化？

​	在Gradient boosting算法中引入一个随机化参数，提出Stochastic Gradient Boosting SGBT算法，因此SGBT可以看做是bagging和boosting的综合体。

​	主要思想：在算法的每一次迭代过程中，随机抽取训练样本的一部分来拟合基础分类器，抽样比例是一个随机化参数f，当f=1时，算法等同于原始的Gradient Boost算法，较小的f值表示用于训练的样本更少，但能够有效的避免过拟合。



##### 18、GBDT优缺点

优点：

- 可以灵活处理各种类型的数据，包括连续值和离散值
- 在相对少的调参时间情况下，预测的准确率也可以比较高。
- 使用一些健壮的损失函数，对异常值的鲁棒性非常强，比如Huber和Quantile损失函数

缺点：

- 由于弱学习器之间存在依赖关系，难以并行训练数据，不过可以通过SGBT来达到并行训练。



##### 19、Adaboost与GBDT相同点与不同点

相同点：

- 算法基本思想：两者都是通过对数据不断迭代训练产生多个分类器，最后对所有分类器进行线性组合
- 权重形式：算法建立过程中有两种权重，样本权重和分类器权重，每一次训练时，每一个样本都被赋予一个权重以此来训练分类器，且每次训练结束后，每个分类器会被赋予相应的权重。

不同点：

- 每个分类器之间关系：Adaboost的最终分类结果是各个分类器的加权平均后得出，而GBDT算法训练过程，每个分类器都是在残差梯度方向上，而梯度由上一轮得到的模型给出，所以与上一次模型息息相关。
- 权重调节：Adaboost每次根据训练产生的分类器误判率来调节样本权重和分类器权重；而GBDT算法则是根据损失函数最小化原则实现样本权重和分类器权重的调节。


